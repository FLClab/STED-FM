{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OorQcjmGTNw"
      },
      "source": [
        "# STED-FM for Super-Resolution Microscopy\n",
        "\n",
        "In this notebook we will go through the steps of installing and using the pretrained STED-FM model. We will also demonstrate how to use the model for super-resolution microscopy on a sample image. \n",
        "\n",
        "We will use the `STED-FM` repository, which provides a PyTorch implementation of the STED-FM model.\n",
        "\n",
        "The notebook will cover the following steps:\n",
        "1. **Installation**: Clone the STED-FM repository and install the required dependencies.\n",
        "2. **Loading the Model**: Load the pretrained STED-FM model.\n",
        "3. **Image Retrieval**: Retrieve a sample image from the SO dataset.\n",
        "4. **Segmentation**: Use the STED-FM model to tarin a segmentation model.\n",
        "5. **Image Generation**: Use the trained model to generate an image from the sample image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zr1CYSYmGtLj"
      },
      "source": [
        "## Installation\n",
        "\n",
        "This notebook requires the `STED-FM` repository to be cloned and installed. The notebook will check if the repository is already cloned, and if not, it will clone it. After cloning, it will install the required packages.\n",
        "\n",
        "The kernel will automatically relaunch after the packages have been installed. You can ignore the warning about the session crashing, as it is necessary for the installation to take effect. The installation may take some minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhYdv3CaGOfd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if os.path.isdir(\"./STED-FM\"):\n",
        "    !git -C ./STED-FM/ pull\n",
        "else:\n",
        "    !git clone https://github.com/FLClab/STED-FM.git\n",
        "%pip install -e ./STED-FM\n",
        "exit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ws3xnqHYHpFG"
      },
      "source": [
        "## Loading the model\n",
        "\n",
        "In this section, we will load the pretrained STED-FM model. The model is pretrained on a dataset of super-resolution microscopy images. The model will be automatically downloaded if it is not already present.\n",
        "\n",
        "The `global_pool` argument can be set to `patch` to have access to the predicted embeddings for each image patch.\n",
        "```python\n",
        "model, cfg = get_pretrained_model_v2(\n",
        "    name = \"mae-lightning-small\",\n",
        "    weights = \"MAE_SMALL_STED\",\n",
        "    as_classifier=True,\n",
        "    global_pool=\"patch\"  # Set to \"patch\" to access predicted embeddings for each image patch    \n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIHeTlpsHeG9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from stedfm import get_pretrained_model_v2\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model, cfg = get_pretrained_model_v2(\n",
        "    name = \"mae-lightning-small\",\n",
        "    weights = \"MAE_SMALL_STED\",\n",
        "    as_classifier=True\n",
        ")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    img = torch.randn(1, 1, 224, 224).to(device)\n",
        "    out = model.forward_features(img) # (1, 384)\n",
        "    print(out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xdl8P3bKRNl0"
      },
      "source": [
        "## Image Retrieval\n",
        "\n",
        "In the STED-FM paper, we performed an image retrieval task on the SO dataset. The model was used to retrieve images based on their embeddings. In the following section, we will demonstrate how to perform this task using the pretrained model.\n",
        "\n",
        "We will first download the SO dataset and then use the model to retrieve images based on their embeddings. The embeddings will be used to compute the similarity between images, and the most similar images will be retrieved. \n",
        "\n",
        "The retrieval performance will be evaluated using the area under the curve (AUC) metric.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUL20YvkrAet"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Everything is relative to this BASE_PATH in the code\n",
        "from stedfm.DEFAULTS import BASE_PATH\n",
        "home = BASE_PATH\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "!mkdir -p {home}/evaluation-data\n",
        "if not os.path.isfile(os.path.join(home, \"evaluation-data\", \"optim-data.zip\")):\n",
        "    !wget -O {home}/evaluation-data/optim-data.zip https://s3.valeria.science/flclab-foundation-models/evaluation-data/optim-data.zip\n",
        "\n",
        "if not os.path.isdir(os.path.join(home, \"evaluation-data\", \"optim-data\")):\n",
        "    !unzip {home}/evaluation-data/optim-data.zip -d {home}/evaluation-data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9_JOlalKscJ"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from matplotlib import pyplot\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "from tqdm.auto import trange\n",
        "from stedfm.loaders import get_dataset\n",
        "\n",
        "metric = \"auc\"\n",
        "\n",
        "_, _, test_loader = get_dataset(name=\"optim\")\n",
        "\n",
        "# Embed dataset\n",
        "embeddings, labels, dataset_indices = [], [], []\n",
        "N = len(test_loader.dataset)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for n in trange(N):\n",
        "        img = test_loader.dataset[n][0].unsqueeze(0).to(device)\n",
        "        metadata = test_loader.dataset[n][1]\n",
        "        label = metadata[\"label\"]\n",
        "        dataset_idx = metadata[\"dataset-idx\"]\n",
        "        output = model.forward_features(img)\n",
        "        embeddings.append(output)\n",
        "        labels.append(label)\n",
        "        dataset_indices.append(dataset_idx)\n",
        "embeddings = torch.cat(embeddings, dim=0)\n",
        "labels = numpy.array(labels)\n",
        "dataset_indices = numpy.array(dataset_indices)\n",
        "assert embeddings.shape[0] == labels.shape[0] == dataset_indices.shape[0]\n",
        "\n",
        "average_precision = []\n",
        "for e in trange(embeddings.shape[0]):\n",
        "    curr_embedding = embeddings[e]\n",
        "    curr_label = labels[e]\n",
        "    curr_dataset_idx = dataset_indices[e]\n",
        "    similarities = F.cosine_similarity(embeddings, curr_embedding.unsqueeze(0), dim=1).cpu().numpy()\n",
        "    sorted_indices = numpy.argsort(similarities)[::-1]\n",
        "\n",
        "    query_labels = []\n",
        "\n",
        "    ## AUC\n",
        "    for w in sorted_indices:\n",
        "        data_index = dataset_indices[w]\n",
        "        query_labels.append(1 if labels[w] == curr_label else 0)\n",
        "    if numpy.unique(query_labels).shape[0] == 1 and query_labels[0] == 1:\n",
        "        auc = 1.0\n",
        "    elif numpy.unique(query_labels).shape[0] == 1 and query_labels[0] == 0:\n",
        "        auc = 0.0\n",
        "    else:\n",
        "        if metric == \"auc\":\n",
        "            auc = roc_auc_score(query_labels, similarities[sorted_indices])\n",
        "        elif metric == \"aupr\":\n",
        "            auc = average_precision_score(query_labels, similarities[sorted_indices])\n",
        "    average_precision.append(auc)\n",
        "\n",
        "print(\"AUROC:\", numpy.mean(average_precision))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXwf6cnItO9m"
      },
      "source": [
        "## Segmentation Experiment\n",
        "\n",
        "In this section, we will train a segmentation model using the STED-FM model. The model will be trained on a dataset of super-resolution microscopy images, the F-actin dataset. The model will be used to segment the images into F-actin rings and fibers. \n",
        "\n",
        "We will first download the F-actin dataset and then use the STED-FM model to train a segmentation model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBdeLHE7tR0n"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy\n",
        "import torch\n",
        "\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# Everything is relative to this BASE_PATH in the code\n",
        "from stedfm.DEFAULTS import BASE_PATH\n",
        "home = BASE_PATH\n",
        "\n",
        "!mkdir -p {home}/segmentation-data\n",
        "if not os.path.isfile(os.path.join(home, \"segmentation-data\", \"factin-segmentation-data.zip\")):\n",
        "    !wget -O {home}/segmentation-data/factin-segmentation-data.zip https://s3.valeria.science/flclab-foundation-models/segmentation-data/factin-segmentation-data.zip\n",
        "\n",
        "if not os.path.isdir(os.path.join(home, \"segmentation-data\", \"factin\")):\n",
        "    !unzip {home}/segmentation-data/factin-segmentation-data.zip -d {home}/segmentation-data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the next cell we will import the necessary libraries and set up the configuration for the segmentation experiment. The configuration will include parameters such as the number of epochs, learning rate, and batch size. \n",
        "\n",
        "The parameters that can be modified are contained in the section\n",
        "```python\n",
        "####################################################\n",
        "##################### <UPDATE> #####################\n",
        "####################################################\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "...\n",
        "\n",
        "####################################################\n",
        "##################### </UPDATE> #####################\n",
        "####################################################\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zzzTH36uta5"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "insert_to_path = \"./STED-FM/experiments/segmentation-experiments\"\n",
        "while insert_to_path not in sys.path:\n",
        "    sys.path.insert(0, insert_to_path)\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy\n",
        "import torch\n",
        "import pickle\n",
        "import shutil\n",
        "import time\n",
        "import json\n",
        "\n",
        "from collections import defaultdict\n",
        "from torch.utils.data import SubsetRandomSampler\n",
        "from typing import Any\n",
        "from tqdm.auto import tqdm\n",
        "from lightly.utils.scheduler import CosineWarmupScheduler\n",
        "\n",
        "from main import intensity_scale_\n",
        "from eval import evaluate_segmentation\n",
        "from datasets import get_dataset\n",
        "\n",
        "from stedfm import get_decoder\n",
        "from stedfm import get_pretrained_model_v2\n",
        "from stedfm.utils import update_cfg, save_cfg, track_loss\n",
        "from stedfm.configuration import Configuration\n",
        "from stedfm.DEFAULTS import BASE_PATH\n",
        "\n",
        "####################################################\n",
        "##################### <UPDATE> #####################\n",
        "####################################################\n",
        "\n",
        "SAVE_FOLDER = \"/content/segmentation-baselines\"\n",
        "RANDOM_SEED = 42\n",
        "USE_TENSORBOARD = False\n",
        "LABEL_PERCENTAGE = 1.0\n",
        "\n",
        "class SegmentationConfiguration(Configuration):\n",
        "\n",
        "    freeze_backbone: bool = True\n",
        "    num_epochs: int = 10\n",
        "    learning_rate: float = 1e-4\n",
        "    batch_size: int = 32\n",
        "\n",
        "####################################################\n",
        "##################### <UPDATE> #####################\n",
        "####################################################\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def validation_step(model: torch.nn.Module, valid_loader: torch.utils.data.DataLoader, criterion: torch.nn.Module, epoch: int, device: torch.device, writer: torch.utils.tensorboard.SummaryWriter = None):\n",
        "    is_training = model.training\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    statLossTest = []\n",
        "    for i, (X, y) in enumerate(tqdm(valid_loader, desc=\"[----] \")):\n",
        "\n",
        "        # Reshape\n",
        "        if isinstance(X, (list, tuple)):\n",
        "            X = [_X.unsqueeze(0) if _X.dim() == 2 else _X for _X in X]\n",
        "        else:\n",
        "            if X.dim() == 3:\n",
        "                X = X.unsqueeze(1)\n",
        "\n",
        "        # Send to gpu\n",
        "        X = X.to(torch.float32)\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # Prediction and loss computation\n",
        "        pred = model.forward(X)\n",
        "        loss = criterion(pred, y)\n",
        "\n",
        "        # Keeping track of statistics\n",
        "        statLossTest.append(loss.item())\n",
        "\n",
        "        if (i == 0) and USE_TENSORBOARD:\n",
        "            writer.add_images(\"Images-test/image\", intensity_scale_(X[:16]), epoch, dataformats=\"NCHW\")\n",
        "            for i in range(cfg.dataset_cfg.num_classes):\n",
        "                writer.add_images(f\"Images-test/label-{i}\", y[:16, i:i+1], epoch, dataformats=\"NCHW\")\n",
        "                writer.add_images(f\"Images-test/pred-{i}\", pred[:16, i:i+1], epoch, dataformats=\"NCHW\")\n",
        "\n",
        "        # To avoid memory leak\n",
        "        torch.cuda.empty_cache()\n",
        "        del X, y, pred, loss\n",
        "\n",
        "    if is_training:\n",
        "        model.train()\n",
        "    return statLossTest\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIdwew7Swtw3"
      },
      "outputs": [],
      "source": [
        "numpy.random.seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "backbone, cfg = get_pretrained_model_v2(\n",
        "    name = \"mae-lightning-small\",\n",
        "    weights = \"MAE_SMALL_STED\",\n",
        ")\n",
        "\n",
        "training_dataset, validation_dataset, testing_dataset = get_dataset(\n",
        "    name=\"factin\",\n",
        "    cfg=cfg,\n",
        "    use_cache=False\n",
        ")\n",
        "\n",
        "segmentation_cfg = SegmentationConfiguration()\n",
        "for key, values in segmentation_cfg.__dict__.items():\n",
        "    setattr(cfg, key, values)\n",
        "print(f\"Config: {cfg.__dict__}\")\n",
        "\n",
        "probe = \"pretrained\"\n",
        "model_name = \"pretrained-\"\n",
        "if cfg.freeze_backbone:\n",
        "    probe = \"pretrained-frozen\"\n",
        "    model_name += \"frozen-\"\n",
        "model_name += \"MAE_SMALL_STED\"\n",
        "if LABEL_PERCENTAGE < 1.0:\n",
        "    model_name += f\"-{int(LABEL_PERCENTAGE * 100)}%-labels\"\n",
        "model_name += f\"-{RANDOM_SEED}\"\n",
        "\n",
        "OUTPUT_FOLDER = os.path.join(SAVE_FOLDER, \"mae-lightning-small\", \"factin\", model_name)\n",
        "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
        "\n",
        "# Saves and prints the configuration\n",
        "cfg.save(os.path.join(OUTPUT_FOLDER, \"config.json\"))\n",
        "print(cfg)\n",
        "\n",
        "# Build the UNet model\n",
        "model = get_decoder(backbone, cfg)\n",
        "model = model.to(device)\n",
        "\n",
        "stats = defaultdict(list)\n",
        "min_valid_loss = numpy.inf\n",
        "start_epoch = 0\n",
        "\n",
        "sampler = None\n",
        "if LABEL_PERCENTAGE < 1.0:\n",
        "    rng = numpy.random.default_rng(RANDOM_SEED)\n",
        "    indices = list(range(len(training_dataset)))\n",
        "    rng.shuffle(indices)\n",
        "    split = int(numpy.floor(LABEL_PERCENTAGE * len(training_dataset)))\n",
        "    train_indices = indices[:split]\n",
        "    sampler = SubsetRandomSampler(train_indices)\n",
        "\n",
        "print(\"----------------------------------------\")\n",
        "print(\"Training Dataset\")\n",
        "print(\"Dataset size: \", len(training_dataset))\n",
        "print(\"Dataset size (with sampler): \", len(sampler) if sampler else len(training_dataset))\n",
        "print(\"----------------------------------------\")\n",
        "print(\"Validation Dataset\")\n",
        "print(\"Dataset size: \", len(validation_dataset))\n",
        "print(f\"Batch size: {cfg.batch_size}\")\n",
        "print(\"----------------------------------------\")\n",
        "\n",
        "# Build a PyTorch dataloader.\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    training_dataset,  # Pass the dataset to the dataloader.\n",
        "    batch_size=cfg.batch_size,  # A large batch size helps with the learning.\n",
        "    shuffle=sampler is None,  # Shuffling is important!\n",
        "    num_workers=int(os.cpu_count()),\n",
        "    sampler=sampler, drop_last=False\n",
        ")\n",
        "valid_loader = torch.utils.data.DataLoader(\n",
        "    validation_dataset,  # Pass the dataset to the dataloader.\n",
        "    batch_size=cfg.batch_size,  # A large batch size helps with the learning.\n",
        "    shuffle=True,  # Shuffling is important!\n",
        "    num_workers=int(os.cpu_count())\n",
        ")\n",
        "\n",
        "# Defines the training budget\n",
        "num_epochs = cfg.num_epochs\n",
        "if LABEL_PERCENTAGE < 1.0:\n",
        "    budget = len(training_dataset) * num_epochs\n",
        "    num_epochs = int(budget / (LABEL_PERCENTAGE * len(training_dataset)))\n",
        "    cfg.num_epochs = num_epochs\n",
        "    print(f\"Training budget is updated: {cfg.num_epochs} epochs\")\n",
        "\n",
        "if cfg.num_epochs > 1000:\n",
        "    cfg.num_epochs = 1000\n",
        "\n",
        "# Defines the optimizer\n",
        "if probe == \"pretrained-frozen\":\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "    scheduler = CosineWarmupScheduler(\n",
        "        optimizer=optimizer, warmup_epochs=0.1*cfg.num_epochs, max_epochs=cfg.num_epochs,\n",
        "        start_value=1.0, end_value=0.01,\n",
        "        period=cfg.num_epochs//10\n",
        "    )\n",
        "else:\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.05, betas=(0.9, 0.999))\n",
        "    scheduler = CosineWarmupScheduler(\n",
        "        optimizer=optimizer, warmup_epochs=0.1*cfg.num_epochs, max_epochs=cfg.num_epochs,\n",
        "        start_value=1.0, end_value=0.01\n",
        "    )\n",
        "\n",
        "criterion = getattr(torch.nn, cfg.dataset_cfg.criterion)()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ojib6JVk1UMP"
      },
      "source": [
        "Once the model and the training is configured, we can proceed to train the model on the F-actin dataset. The model will be trained for a specified number of epochs, and the training progress will be logged."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7G6QwyvD620H"
      },
      "outputs": [],
      "source": [
        "step = start_epoch * len(train_loader)\n",
        "\n",
        "writer = None\n",
        "if USE_TENSORBOARD:\n",
        "    from torch.utils.tensorboard import SummaryWriter\n",
        "    writer = SummaryWriter(log_dir=os.path.join(OUTPUT_FOLDER, \"logs\"))\n",
        "\n",
        "for epoch in range(start_epoch, cfg.num_epochs):\n",
        "\n",
        "    start = time.time()\n",
        "    print(\"[----] Starting epoch {}/{}\".format(epoch + 1, cfg.num_epochs))\n",
        "\n",
        "    # Keep track of the loss of train and test\n",
        "    statLossTrain = []\n",
        "\n",
        "    # Puts the model in training mode\n",
        "    model.train()\n",
        "    for i, (X, y) in enumerate(tqdm(train_loader, desc=\"[----] \")):\n",
        "\n",
        "        # Reshape\n",
        "        if isinstance(X, (list, tuple)):\n",
        "            X = [_X.unsqueeze(0) if _X.dim() == 2 else _X for _X in X]\n",
        "        else:\n",
        "            if X.dim() == 3:\n",
        "                X = X.unsqueeze(1)\n",
        "\n",
        "        # Send to gpu\n",
        "        X = X.to(torch.float32)\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # Prediction and loss computation\n",
        "        pred = model.forward(X)\n",
        "        loss = criterion(pred, y)\n",
        "\n",
        "        # Keeping track of statistics\n",
        "        statLossTrain.append(loss.item())\n",
        "\n",
        "        # Back-propagation and optimizer step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i == 0) and USE_TENSORBOARD:\n",
        "            writer.add_images(\"Images-train/image\", intensity_scale_(X[:16]), epoch, dataformats=\"NCHW\")\n",
        "            for i in range(cfg.dataset_cfg.num_classes):\n",
        "                writer.add_images(f\"Images-train/label-{i}\", y[:16, i:i+1], epoch, dataformats=\"NCHW\")\n",
        "                writer.add_images(f\"Images-train/pred-{i}\", pred[:16, i:i+1], epoch, dataformats=\"NCHW\")\n",
        "\n",
        "        # To avoid memory leak\n",
        "        torch.cuda.empty_cache()\n",
        "        del X, y, pred, loss\n",
        "\n",
        "        # Puts the model in evaluation mode\n",
        "        if step % int(25 * 32 / cfg.batch_size) == 0:\n",
        "            # Validation step\n",
        "            statLossTest = validation_step(model, valid_loader, criterion, epoch, device, writer)\n",
        "            for key, func in zip((\"testMean\", \"testMed\", \"testMin\", \"testStd\"),\n",
        "                                    (numpy.mean, numpy.median, numpy.min, numpy.std)):\n",
        "                stats[key].append(func(statLossTest))\n",
        "                if USE_TENSORBOARD:\n",
        "                    writer.add_scalar(f\"Loss/{key}\", stats[key][-1], step)\n",
        "            stats[\"testStep\"].append(step)\n",
        "        step += 1\n",
        "\n",
        "    # Aggregate stats\n",
        "    for key, func in zip((\"trainMean\", \"trainMed\", \"trainMin\", \"trainStd\"),\n",
        "            (numpy.mean, numpy.median, numpy.min, numpy.std)):\n",
        "        stats[key].append(func(statLossTrain))\n",
        "        if USE_TENSORBOARD:\n",
        "            writer.add_scalar(f\"Loss/{key}\", stats[key][-1], step)\n",
        "\n",
        "    # scheduler.step(numpy.min(stats[\"testMean\"]))\n",
        "    scheduler.step()\n",
        "    stats[\"lr\"].append(scheduler.get_last_lr())\n",
        "    if USE_TENSORBOARD:\n",
        "        _lr = stats[\"lr\"][-1]\n",
        "        if isinstance(_lr, list):\n",
        "            for i in range(len(_lr)):\n",
        "                writer.add_scalar(f\"Learning-rate/lr-{i}\", _lr[i], step)\n",
        "        else:\n",
        "            writer.add_scalar(f\"Learning-rate/lr\", _lr, step)\n",
        "        writer.add_scalar(f\"Epochs/epoch\", epoch, step)\n",
        "    stats[\"trainStep\"].append(step)\n",
        "\n",
        "    track_loss(\n",
        "        train_loss=stats[\"trainMean\"],\n",
        "        val_loss=stats[\"testMean\"],\n",
        "        val_acc=stats[\"testAcc\"],\n",
        "        lrates=stats[\"lr\"],\n",
        "        save_dir=os.path.join(OUTPUT_FOLDER, \"training-curves.png\")\n",
        "    )\n",
        "    # Save if best model so far\n",
        "    if min_valid_loss > stats[\"testMean\"][-1]:\n",
        "        min_valid_loss = stats[\"testMean\"][-1]\n",
        "        savedata = {\n",
        "            \"model\" : model.state_dict(),\n",
        "            \"optimizer\" : optimizer.state_dict(),\n",
        "            \"stats\" : stats,\n",
        "        }\n",
        "        torch.save(\n",
        "            savedata,\n",
        "            os.path.join(OUTPUT_FOLDER, \"result.pt\"))\n",
        "\n",
        "        del savedata\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Niyxexm9zR_R"
      },
      "source": [
        "Once the model is trained, we will use it to segment a sample image from the F-actin dataset. The segmentation results will be visualized to demonstrate the performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FeqyFaVzK4s"
      },
      "outputs": [],
      "source": [
        "# Build the UNet model.\n",
        "model = get_decoder(backbone, cfg)\n",
        "ckpt = torch.load(os.path.join(OUTPUT_FOLDER, \"result.pt\"), weights_only=False)[\"model\"]\n",
        "print(\"Restoring model...\")\n",
        "model.load_state_dict(ckpt)\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Build a PyTorch dataloader.\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    testing_dataset,  # Pass the dataset to the dataloader.\n",
        "    batch_size=cfg.batch_size,  # A large batch size helps with the learning.\n",
        "    shuffle=True,  # Shuffling is important!\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "scores = evaluate_segmentation(model, test_loader, savefolder=None, device=device, dataset_name=\"factin\")\n",
        "with open(os.path.join(OUTPUT_FOLDER, \"segmentation-scores.json\"), \"w\") as file:\n",
        "    json.dump(scores, file, indent=4)\n",
        "\n",
        "for key, values in scores.items():\n",
        "    print(\"Results for\", key)\n",
        "    values = numpy.array(values)\n",
        "\n",
        "    fig, ax = pyplot.subplots(figsize=(3, 3))\n",
        "    for i in range(values.shape[1]):\n",
        "        data = values[:, i]\n",
        "\n",
        "        # Remove -1 values as they are not valid\n",
        "        data = data[data != -1]\n",
        "\n",
        "        print(\n",
        "                \"avg : {:0.4f}\".format(numpy.mean(data, axis=0)),\n",
        "                \"std : {:0.4f}\".format(numpy.std(data, axis=0)),\n",
        "                \"med : {:0.4f}\".format(numpy.median(data, axis=0)),)\n",
        "\n",
        "        bplot = ax.boxplot(data, positions=[i], widths=0.8)\n",
        "        for element in ['boxes', 'whiskers', 'fliers', 'means', 'medians', 'caps']:\n",
        "            pyplot.setp(bplot[element], color='black')\n",
        "\n",
        "    ax.set(\n",
        "        xticks = numpy.arange(values.shape[1]), xticklabels=testing_dataset.classes,\n",
        "        ylim = (0, 1)\n",
        "    )\n",
        "    pyplot.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVWhofpmihSE"
      },
      "source": [
        "## Diffusion Experiment\n",
        "\n",
        "In the STED-FM paper, we trained a diffusion model that was conditioned on the STED-FM embeddings. In this section, we will use the pretrained diffusion model to generate images from image embeddings. We will use the images available from this dataset\n",
        "> Deschênes, A., Santiague, J.-G. S., & Lavoie-Cardinal, F. (2025). Confocal- and STED-FLIM images of neuronal proteins [Data set]. Zenodo. https://doi.org/10.5281/zenodo.15438495"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZnv9iGHxhlv"
      },
      "outputs": [],
      "source": [
        "# Download the model\n",
        "import os\n",
        "import sys\n",
        "import numpy\n",
        "import torch\n",
        "\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# Everything is relative to this BASE_PATH in the code\n",
        "from stedfm.DEFAULTS import BASE_PATH\n",
        "home = BASE_PATH\n",
        "\n",
        "!mkdir -p {home}/baselines\n",
        "if not os.path.isfile(os.path.join(home, \"baselines\", \"diffusion-model.zip\")):\n",
        "    !wget -O {home}/baselines/diffusion-model.zip https://s3.valeria.science/flclab-foundation-models/models/diffusion-model.zip\n",
        "\n",
        "model_path = os.path.join(home, \"baselines\", \"DiffusionModels\", \"latent-guidance\")\n",
        "if not os.path.isdir(model_path):\n",
        "    !unzip {home}/baselines/diffusion-model.zip -d {home}/baselines\n",
        "\n",
        "# Download the data\n",
        "!mkdir -p {home}/evaluation-data\n",
        "if not os.path.isfile(os.path.join(home, \"evaluation-data\", \"low-high-quality.zip\")):\n",
        "    !wget -O {home}/evaluation-data/low-high-quality.zip https://s3.valeria.science/flclab-foundation-models/evaluation-data/low-high-quality.zip\n",
        "\n",
        "if not os.path.isdir(os.path.join(home, \"evaluation-data\", \"low-high-quality\")):\n",
        "    !unzip {home}/evaluation-data/low-high-quality.zip -d {home}/evaluation-data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DNXhlblgipg"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "insert_to_path = \"./STED-FM/experiments/diffusion-experiments\"\n",
        "while insert_to_path not in sys.path:\n",
        "    sys.path.insert(0, insert_to_path)\n",
        "\n",
        "from typing import Union\n",
        "from diffusion_models.diffusion.ddpm_lightning import DDPM\n",
        "from diffusion_models.diffusion.denoising.unet import UNet\n",
        "from attribute_datasets import LowHighResolutionDataset\n",
        "\n",
        "def denormalize(img: Union[numpy.ndarray, torch.Tensor], mu: float = 0.010903545655310154, std: float = 0.03640301525592804) -> Union[numpy.ndarray, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Denormalizes an image. Note that the parameters mu and sigma seem hard-coded but they have been computed from the training sets and can be found\n",
        "    in the attribute_datasets.py file.\n",
        "    \"\"\"\n",
        "    return img * std + mu\n",
        "\n",
        "latent_encoder, model_config = get_pretrained_model_v2(\n",
        "    name=\"mae-lightning-small\",\n",
        "    weights=\"MAE_SMALL_STED\",\n",
        "    as_classifier=True,\n",
        ")\n",
        "denoising_model = UNet(\n",
        "    dim=64,\n",
        "    channels=1,\n",
        "    dim_mults=(1,2,4),\n",
        "    cond_dim=model_config.dim,\n",
        "    condition_type=\"latent\",\n",
        "    num_classes=4\n",
        ")\n",
        "diffusion_model = DDPM(\n",
        "    denoising_model=denoising_model,\n",
        "    timesteps=1000,\n",
        "    beta_schedule=\"linear\",\n",
        "    condition_type=\"latent\",\n",
        "    latent_encoder=latent_encoder,\n",
        ")\n",
        "\n",
        "ckpt = torch.load(os.path.join(model_path, \"MAE_SMALL_STED\", \"checkpoint-69.pth\"))\n",
        "diffusion_model.load_state_dict(ckpt[\"state_dict\"])\n",
        "diffusion_model.to(device)\n",
        "diffusion_model.eval()\n",
        "\n",
        "dataset = LowHighResolutionDataset(\n",
        "    h5path=os.path.join(home, \"evaluation-data\", \"low-high-quality\", \"testing.hdf5\"),\n",
        "    num_samples=None,\n",
        "    transform=None,\n",
        "    n_channels=1,\n",
        "    num_classes=2,\n",
        "    classes=[\"low\", \"high\"]\n",
        ")\n",
        "print(len(dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the next cell, we will select an image from the dataset and use the diffusion model to generate a new image based on the selected image's embedding. The generated image will be visualized to demonstrate the performance of the diffusion model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jinmfb4U84Za"
      },
      "outputs": [],
      "source": [
        "index = 0 # Change this index to select a different image\n",
        "img, metadata = dataset[index]\n",
        "label = metadata[\"label\"]\n",
        "\n",
        "img = img.clone().unsqueeze(0).to(device)\n",
        "latent_code = diffusion_model.latent_encoder.forward_features(img)\n",
        "\n",
        "sample = diffusion_model.p_sample_loop(\n",
        "    shape=(img.shape[0], 1, img.shape[2], img.shape[3]),\n",
        "    cond=latent_code,\n",
        "    progress=True\n",
        ")\n",
        "sample = denormalize(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mx8zdmP9p5Y"
      },
      "outputs": [],
      "source": [
        "fig, axes = pyplot.subplots(1, 2, figsize=(10, 5))\n",
        "axes[0].imshow(img.squeeze().cpu().numpy(), cmap=\"hot\")\n",
        "axes[1].imshow(sample.squeeze().cpu().numpy(), cmap=\"hot\")\n",
        "axes[0].set_title(f\"Original Image (Label: {label})\")\n",
        "axes[1].set_title(\"Generated Sample\")\n",
        "for ax in axes:\n",
        "    ax.axis(\"off\")\n",
        "pyplot.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

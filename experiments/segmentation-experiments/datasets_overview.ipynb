{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import get_dataset\n",
    "import sys \n",
    "import matplotlib.pyplot as plt\n",
    "from tiffwrapper import make_composite\n",
    "from decoders import get_decoder\n",
    "import torch\n",
    "import os\n",
    "sys.path.insert(0, \"../\")\n",
    "from model_builder import get_base_model, get_pretrained_model_v2\n",
    "from configuration import Configuration \n",
    "from DEFAULTS import BASE_PATH\n",
    "DATASET = \"synaptic-semantic-segmentation\"\n",
    "MODEL = \"mae-lightning-small\"\n",
    "WEIGHTS = \"MAE_SMALL_STED\"\n",
    "CHECKPOINT = \"/home-local/Frederic/segmentation-baselines/mae-lightning-small/factin/pretrained-MAE_SMALL_STED-46\" \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_channels 1\n",
      "--- mae-lightning-small | /home-local/Frederic/baselines/mae-small_STED/pl_checkpoint-999.pth ---\n",
      "\n",
      "--- Loaded model mae-lightning-small with weights MAE_SMALL_STED ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3678/3678 [00:03<00:00, 1052.63it/s]\n",
      "100%|██████████| 1102/1102 [00:01<00:00, 1031.23it/s]\n",
      "100%|██████████| 1357/1357 [00:01<00:00, 1102.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 1191\n",
      "Valid dataset size: 354\n",
      "Test dataset size: 375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "backbone, cfg = get_pretrained_model_v2(MODEL, WEIGHTS)\n",
    "train_dataset, _, _ = get_dataset(name=DATASET, cfg=cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SegmentationConfiguration(Configuration):\n",
    "    \n",
    "#     freeze_backbone: bool = True\n",
    "#     num_epochs: int = 300\n",
    "#     learning_rate: float = 1e-4\n",
    "# segmentation_cfg = SegmentationConfiguration()\n",
    "# for key, value in segmentation_cfg.__dict__.items():\n",
    "#         setattr(cfg, key, value)\n",
    "# model = get_decoder(backbone, cfg)\n",
    "# ckpt = torch.load(os.path.join(CHECKPOINT, \"result.pt\"))[\"model\"]\n",
    "# model.load_state_dict(ckpt)\n",
    "# model = model.to(DEVICE)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]/opt/anaconda3/envs/phd-env/lib/python3.8/site-packages/tiffwrapper/utils.py:106: RuntimeWarning: invalid value encountered in divide\n",
      "  scaled = (image - m[:, numpy.newaxis, numpy.newaxis]) / (M - m)[:, numpy.newaxis, numpy.newaxis]\n",
      "/opt/anaconda3/envs/phd-env/lib/python3.8/site-packages/tiffwrapper/utils.py:84: RuntimeWarning: invalid value encountered in cast\n",
      "  ary = ary.astype(numpy.uint8)\n",
      "100%|██████████| 20/20 [02:45<00:00,  8.26s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def merge_masks(masks):\n",
    "    image_rgb = make_composite(masks, luts=[\"green\", \"magenta\", \"cyan\", \"yellow\"], ranges=[(masks[i].min(), masks[i].max()) for i in range(masks.shape[0])])\n",
    "    return image_rgb\n",
    "\n",
    "N = len(train_dataset)\n",
    "indices = np.random.randint(0, N, size=20)\n",
    "\n",
    "for i in tqdm(indices, total=len(indices)):\n",
    "    image, mask = train_dataset[i]\n",
    "    # pred = model(image.unsqueeze(0).to(DEVICE)).squeeze().detach().cpu().numpy()\n",
    "    image = image.squeeze().cpu().numpy()\n",
    "    mask = mask.cpu().numpy()\n",
    "    mask = merge_masks(mask)\n",
    "    # pred = merge_masks(pred)\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    axs[0].imshow(image, cmap=\"hot\", vmax=1.2)\n",
    "    axs[1].imshow(mask)\n",
    "    # axs[2].imshow(pred)\n",
    "    # axs[0].set_title(i)\n",
    "    for ax in axs:\n",
    "        ax.axis(\"off\")\n",
    "    fig.savefig(f\"./dummy_images/{DATASET}_example_{i}.pdf\", bbox_inches=\"tight\", dpi=1200)\n",
    "    plt.close(fig)\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

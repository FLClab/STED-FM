{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import DataLoader \n",
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "from typing import List, Optional, Tuple\n",
    "from stedfm import get_pretrained_model_v2\n",
    "from stedfm.DEFAULTS import BASE_PATH \n",
    "from stedfm.loaders import get_dataset \n",
    "from torchvision.models.feature_extraction import get_graph_node_names, create_feature_extractor\n",
    "\n",
    "DATASET = \"optim\"\n",
    "MODEL = \"mae-lightning-small\"\n",
    "WEIGHTS = \"MAE_SMALL_STED\"\n",
    "GLOBAL_POOL = \"avg\"\n",
    "\n",
    "\n",
    "def get_classes(dataset: str):\n",
    "    if dataset == \"optim\":\n",
    "        return [\"Actin\", \"Tubulin\", \"CaMKII\", \"PSD95\"]  \n",
    "    elif dataset == \"neural-activity-states\":\n",
    "        return [\"Block\", \"0Mg\", \"GluGly\", \"48hTTX\"]\n",
    "    elif dataset == \"peroxisome\":\n",
    "        return [\"4hMeOH\", \"6hMeOH\", \"8hMeOH\", \"16hMeOH\"]\n",
    "    elif dataset == \"polymer-rings\":\n",
    "        return [\"CdvB1\", \"CdvB2\"]\n",
    "    elif dataset == \"dl-sim\":\n",
    "        return [\"adhesion\", \"factin\", \"microtubule\", \"mitosis\"]\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset {dataset} not supported\")\n",
    "    \n",
    "CLASSES = get_classes(DATASET)\n",
    "N_CLASSES = len(CLASSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running on cuda ---\n",
      "mask_ratio 0.0\n",
      "pretrained False\n",
      "in_channels 1\n",
      "blocks all\n",
      "num_classes 4\n",
      "--- mae-lightning-small | /home-local/Frederic/baselines/mae-small_STED/pl_checkpoint-999.pth ---\n",
      "\n",
      "--- Loaded model mae-lightning-small with weights MAE_SMALL_STED ---\n",
      "--- ViT case with none-ImageNet weights or from scratch ---\n",
      "--- Freezing every parameter in mae-lightning-small ---\n",
      "--- Added linear probe to all frozen blocks ---\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"--- Running on {DEVICE} ---\")\n",
    "\n",
    "model, cfg = get_pretrained_model_v2(\n",
    "    name=MODEL,\n",
    "    weights=WEIGHTS,\n",
    "    path=None,\n",
    "    mask_ratio=0.0,\n",
    "    pretrained=True if \"imagenet\" in WEIGHTS.lower() else False,\n",
    "    in_channels=3 if \"imagenet\" in WEIGHTS.lower() else 1,\n",
    "    as_classifier=True,\n",
    "    blocks=\"all\",\n",
    "    num_classes=4\n",
    ")\n",
    "model.to(DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home-local/Frederic/evaluation-data/optim_train\n",
      "Train dataset size: 1536 --> (array([0, 1, 2, 3]), array([881, 220, 208, 227]))\n",
      "Valid dataset size: 147 --> (array([0, 1, 2, 3]), array([82, 29, 16, 20]))\n",
      "Test dataset size: 438 --> (array([0, 1, 2, 3]), array([261,  60,  56,  61]))\n"
     ]
    }
   ],
   "source": [
    "_, _, test_loader = get_dataset(\n",
    "    name=DATASET,\n",
    "    transform=None,\n",
    "    training=True,\n",
    "    path=None,\n",
    "    batch_size=cfg.batch_size,\n",
    "    n_channels=3 if \"imagenet\" in WEIGHTS.lower() else 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from timm.models.layers import PatchEmbed \n",
    "import torch.nn.functional as F\n",
    "from tiffwrapper import make_composite\n",
    "\n",
    "def show_amap(image, a_map):\n",
    "    image = image.squeeze().cpu().data.numpy()\n",
    "    a_map = a_map.squeeze().cpu().data.numpy()\n",
    "    if image.ndim == 3:\n",
    "        image = image[0]\n",
    "\n",
    "    m, M = np.min(image), np.max(image)\n",
    "    image_rgb = make_composite(np.array([image]), luts=[\"gray\"], ranges=[(m, M)])\n",
    "    image_amap_rgb = make_composite(np.stack([image, a_map]), luts=[\"gray\", \"Orange Hot\"], ranges=[(m, M), (a_map.min() + 0.25 *(a_map.max() - a_map.min()), a_map.max())])\n",
    "\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(image_amap_rgb)\n",
    "    ax.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "N = len(test_loader.dataset)\n",
    "indices = np.arange(N)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "feature_extractor = create_feature_extractor(\n",
    "    model, return_nodes=[\"backbone.blocks.11.attn.q_norm\", \"backbone.blocks.11.attn.k_norm\"],\n",
    "    tracer_kwargs={'leaf_modules': [PatchEmbed]}\n",
    ")\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    xs = np.arange(0, 224, 16)\n",
    "    print(indices[0])\n",
    "    for i in [272]:\n",
    "        # fig = plt.figure(figsize=(10, 10))\n",
    "        # ax = fig.add_subplot(111)\n",
    "        # img = test_loader.dataset[i][0].unsqueeze(0).to(DEVICE)\n",
    "        img = test_loader.dataset[i][0].squeeze().cpu().numpy()\n",
    "        np.save(\"./img.npy\", img)\n",
    "        # out = feature_extractor(img)\n",
    "        # q, k = out[\"backbone.blocks.11.attn.q_norm\"], out[\"backbone.blocks.11.attn.k_norm\"]\n",
    "        # factor = (384 / 6) ** -0.5 # (head_dim / num_heads ) ** -0.5\n",
    "        # q = q * factor \n",
    "        # attn = q @ k.transpose(-2, -1) # (1, 6, 197, 197)\n",
    "        # attn = attn.softmax(dim=-1) # (1, 6, 197, 197) \n",
    "        # head = np.random.randint(0, 6)\n",
    "        # attn_map = attn[:, head, :, :].squeeze(0)\n",
    "        # for head in range(6):\n",
    "        #     print(head)\n",
    "        #     cls_attn_map = attn[:, :, 0, 1:]  # (1, 6, 196) \n",
    "        #     cls_attn_map = cls_attn_map[:, head, :].view(14, 14).detach() # (14, 14)\n",
    "        #     cls_resized = F.interpolate(cls_attn_map.view(1, 1, 14, 14), (224, 224), mode='bilinear').view(224, 224, 1) # (224, 224, C)\n",
    "        #     m, M = cls_resized.min(), cls_resized.max()\n",
    "        #     cls_resized = (cls_resized - m) / (M - m)\n",
    "        #     show_amap(img, cls_resized)\n",
    "        # img = test_loader.dataset[i][0].squeeze().cpu().numpy()\n",
    "        # ax.imshow(img, cmap=\"hot\")\n",
    "        # ax.set_title(i)\n",
    "        # for x in xs:\n",
    "        #     ax.axvline(x, color=\"white\", linewidth=1.0)\n",
    "        #     ax.axhline(x, color=\"white\", linewidth=1.0)\n",
    "\n",
    "    #     ax.axis(\"off\")\n",
    "    #     plt.show()\n",
    "    #     plt.close(fig)\n",
    "    # fig.savefig(\"./patchified_img.png\", dpi=1200, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mat = np.random.rand(7,7)\n",
    "mat *= 0.7\n",
    "np.fill_diagonal(mat, 1.0)\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.imshow(mat, cmap=\"RdPu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1242, 384]) (1242,)\n"
     ]
    }
   ],
   "source": [
    "embeddings, labels, dataset_idx = [], [], []\n",
    "N = len(test_loader.dataset)\n",
    "with torch.no_grad():\n",
    "    for i in range(N):\n",
    "        img = test_loader.dataset[i][0].unsqueeze(0)\n",
    "        metadata = test_loader.dataset[i][1]\n",
    "        img = img.to(DEVICE)\n",
    "        label = metadata[\"label\"]\n",
    "        d_id = metadata[\"dataset-idx\"]\n",
    "        output = model.forward_features(img)\n",
    "        embeddings.append(output)\n",
    "        labels.append(label)\n",
    "        dataset_idx.append(d_id)\n",
    "\n",
    "embeddings = torch.cat(embeddings, dim=0)\n",
    "labels = np.array(labels)\n",
    "dataset_idx = np.array(dataset_idx)\n",
    "print(embeddings.shape, labels.shape)\n",
    "assert embeddings.shape[0] == labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1242/1242 [00:30<00:00, 40.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average retrieval accuracy: 0.88 ± 0.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "num_repetitions = 50\n",
    "num_samples = 30\n",
    "rep_accuracies = []\n",
    "for n in trange(embeddings.shape[0]):\n",
    "    random_embedding = embeddings[n]\n",
    "    target_label = labels[n]\n",
    "    img = test_loader.dataset[n][0].squeeze().cpu().numpy()\n",
    "    img = img[0] if \"imagenet\" in WEIGHTS.lower() else img\n",
    "    similarities = F.cosine_similarity(embeddings, random_embedding.unsqueeze(0), dim=1).cpu().numpy()\n",
    "    sorted_indices = np.argsort(similarities)[::-1]\n",
    "    query_labels = []\n",
    "\n",
    "    for i in sorted_indices[1:num_samples+1]:\n",
    "        sim = similarities[i]\n",
    "        data_index = dataset_idx[i]\n",
    "        similar_img = test_loader.dataset[data_index][0].squeeze().cpu().numpy()\n",
    "        query_labels.append(labels[i])\n",
    "        similar_img = similar_img[0] if \"imagenet\" in WEIGHTS.lower() else similar_img\n",
    "        # if labels[i] != target_label:\n",
    "        #     fig, axs = plt.subplots(1, 2)\n",
    "        #     axs[0].imshow(img, cmap='hot')\n",
    "        #     axs[1].imshow(similar_img, cmap='hot')\n",
    "        #     axs[1].set_title(f\"Similarity: {sim:.2f}\")\n",
    "        #     for ax in axs:\n",
    "        #         ax.axis('off')\n",
    "        #     plt.show()\n",
    "\n",
    "    retrieval_accuracy = np.sum(np.array(query_labels) == target_label) / len(query_labels)\n",
    "    rep_accuracies.append(retrieval_accuracy)\n",
    "\n",
    "print(f\"Average retrieval accuracy: {np.mean(rep_accuracies):.2f} ± {np.std(rep_accuracies):.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
